{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lxml                # import the lxml library\n",
    "import codecs              # import the codecs library\n",
    "from bs4 import BeautifulSoup    # import the BeautifulSoup class from the bs4 library\n",
    "import pandas as pd        # import the pandas library and give it an alias \"pd\" for easier use\n",
    "from tqdm.notebook import tqdm  # import the tqdm library's notebook module\n",
    "import numpy as np         # import the numpy library and give it an alias \"np\" for easier use\n",
    "import glob                # import the glob library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_file_new = glob.glob(\"/home/harsh.d/arxiv/xml_new/*\")   # creates a list of file paths that match the pattern in the specified directory\n",
    "xml_file_old = glob.glob(\"/home/harsh.d/old_xml_files/*\")       # creates a list of file paths that match the pattern in the specified directory\n",
    "xml_files = xml_file_old + xml_file_new                    # concatenates the two lists of file paths into a single list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "snap_df=pd.read_json(\"/home/harsh.d/Shoaib/arxiv_metadata/arxiv-metadata-oai-snapshot.json\",lines=True)   # read the specified JSON file into a pandas DataFrame\n",
    "snap_df['id']=snap_df['id'].str.replace('/','')    # remove forward slashes from the 'id' column of the DataFrame\n",
    "snap_df['title']=snap_df['title'].str.lower()     # convert the text in the 'title' column of the DataFrame to lowercase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Get Unique Papers based on Title+Authors\"\"\"\n",
    "def get_titles_authors(file):\n",
    "\n",
    "    titles=[]\n",
    "    authors=[]\n",
    "    doi=[]\n",
    "    names=[]\n",
    "    \n",
    "    # Read XML\n",
    "    with codecs.open(file,'r',\"utf-8\") as tei:\n",
    "        soup = BeautifulSoup(tei, 'lxml')\n",
    "    \n",
    "    title=snap_df[snap_df['id']==file.split('/')[-1][:-8]]['title'].values[0].lower()\n",
    "    # Paper Title\n",
    "    if(title==''):\n",
    "        title=soup.teiheader.title.get_text().lower()\n",
    "        \n",
    "#         print(\"Taken from Snap\")\n",
    "    \n",
    "    \n",
    "    # Paper Author Details    \n",
    "    for author in soup.teiheader.find_all('author'):\n",
    "    \n",
    "        if(author.find('persname')!=None):\n",
    "        \n",
    "            if(author.find('persname').forename!=None and author.find('persname').surname!=None):\n",
    "                names.append(author.find('persname').forename.get_text()[0].lower()+\" \"+author.find('persname').surname.get_text().lower())\n",
    "\n",
    "            elif(author.find('persname').forename==None):\n",
    "                names.append(author.find('persname').surname.get_text().lower())\n",
    "\n",
    "            else:\n",
    "                names.append(author.find('persname').forename[0].get_text().lower())\n",
    "                \n",
    "    titles.append(title)\n",
    "    authors.append(names)\n",
    "    \n",
    "    # Dictionary containg <XML ID: Citation> format\n",
    "    citations={}\n",
    "    for targets in soup.find_all('ref'):\n",
    "        if(targets.get('type')=='bibr' and targets.get('target')!=None):\n",
    "            citations[targets.get('target')]=targets.get_text().lower()\n",
    "    \n",
    "    # Extract Citation, Reference Titles, Reference Authors \n",
    "    for ref in soup.back.find_all('biblstruct'):\n",
    "        \n",
    "        if('#'+ref.get(\"xml:id\") in citations):\n",
    "            \n",
    "            cit=citations['#'+ref.get(\"xml:id\")]\n",
    "            \n",
    "            xml_id=ref.get(\"xml:id\")\n",
    "            ref_title = \"\"\n",
    "            for t in ref.find_all('title'):\n",
    "                if(t.get_text()!=''):\n",
    "                    ref_title=t.get_text().lower()\n",
    "                    break\n",
    "            \n",
    "            if(ref.find(type=\"arXiv\")!=None):\n",
    "                ref_doi=ref.find(type=\"arXiv\").get_text().lower()\n",
    "            elif(ref.find(type=\"DOI\")!=None):\n",
    "                ref_doi=ref.find(type=\"DOI\").get_text().lower()\n",
    "            else:\n",
    "                ref_doi=''\n",
    "                \n",
    "#             reference=ref.find_all(type='raw_reference')[0].get_text()\n",
    "\n",
    "            ref_authors=[]\n",
    "            for author in ref.find_all('author'):\n",
    "\n",
    "                if(author.find('persname')!=None):\n",
    "\n",
    "                    if(author.find('persname').forename!=None and author.find('persname').surname!=None):\n",
    "                        ref_authors.append(author.find('persname').forename.get_text()[0].lower()+\" \"+author.find('persname').surname.get_text().lower())\n",
    "\n",
    "                    elif(author.find('persname').forename==None):\n",
    "                        ref_authors.append(author.find('persname').surname.get_text().lower())\n",
    "\n",
    "                    else:\n",
    "                        ref_authors.append(author.find('persname').forename.get_text()[0].lower())\n",
    "                        \n",
    "            titles.append(ref_title)\n",
    "            authors.append(ref_authors)\n",
    "            doi.append(ref_doi)\n",
    "                        \n",
    "    \n",
    "    return file.split('/')[-1],titles,doi,authors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1942302"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xml_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58897d8930d54e8f98dc0d70ca701ac3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1942302 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2a6e55330704774948a59ab722fcbf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1942302 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "outputs=[]\n",
    "\n",
    "def driver_func():\n",
    "    \n",
    "    # Note: Keep Num_Process <= 45 to avoid system hang\n",
    "    PROCESSES = 40\n",
    "    \n",
    "    with multiprocessing.Pool(PROCESSES) as pool:\n",
    "        results=[pool.apply_async(get_titles_authors, args=(p,)) for p in tqdm(xml_files)]\n",
    "        \n",
    "        \n",
    "        for r in tqdm(results):\n",
    "            try:\n",
    "                if(r.get(timeout=5)!=None):\n",
    "                    outputs.append(r.get())\n",
    "                \n",
    "            except Exception as e:\n",
    "                    pass\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "driver_func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4946d78bbbac4cca9c473aa7ae5a6d82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1939589 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_list=[]\n",
    "for f,titles,doi,authors in tqdm(outputs):\n",
    "    output_list+=[{'title':t,'authors':a} for t,d,a in zip(titles,doi,authors)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove papers with name starting with 'under consideration...'\n",
    "temp = pd.DataFrame(output_list)\n",
    "temp.drop(temp[temp['title'].str.startswith('under consideration')].index,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates\n",
    "temp['authors_tup']=temp['authors'].apply(lambda x : tuple(x) if type(x) is list else x)\n",
    "temp.drop_duplicates(subset=['title','authors_tup'],keep='first',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.index.name = 'id'\n",
    "temp.reset_index(inplace=True)\n",
    "temp=temp[['id','title','authors_tup']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title</th>\n",
       "      <th>authors_tup</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>structadmm: a systematic, high-efficiency framework of structured weight\\n  pruning for dnns</th>\n",
       "      <th>(t zhang, s ye, k zhang, x ma, n liu, l zhang, j tang, k ma, x lin, m fardad, y wang)</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>distributed optimization and statistical learning via the alternating direction method of multipliers</th>\n",
       "      <th>([ references, boyd)</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>learning both weights and connections for efficient neural network</th>\n",
       "      <th>()</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>convergence analysis of alternating direction method of multipliers for a family of nonconvex problems</th>\n",
       "      <th>(m luo, z hong, ; luo, hong)</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>learning structured sparsity in deep neural networks</th>\n",
       "      <th>()</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>l1-depth revisited: a robust angle-based outlier factor in high-dimensional space</th>\n",
       "      <th>(n pham,)</th>\n",
       "      <td>62787071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>locality adaptive discriminant analysis framework</th>\n",
       "      <th>(x li, q wang, f nie, m chen)</th>\n",
       "      <td>62787072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>linear discriminant analysis: new formulations and overfit analysis</th>\n",
       "      <th>(d luo, c ding, h huang)</th>\n",
       "      <td>62787074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>robust 2dpca with non-greedy ¡inline-formula¿ ¡tex-math notation=\"latex\"¿ 1 ¡/tex-math¿¡/inline-formula¿-norm maximization for image analysis</th>\n",
       "      <th>(r wang, f nie, x yang, f gao, m yao)</th>\n",
       "      <td>62787079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>generalization bounds for supervised dimensionality reduction</th>\n",
       "      <th>(m mohri, a rostamizadeh, d storcheus)</th>\n",
       "      <td>62787099</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19363735 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                             id\n",
       "title                                              authors_tup                                                 \n",
       "structadmm: a systematic, high-efficiency frame... (t zhang, s ye, k zhang, x ma, n liu, l zhang, ...         0\n",
       "distributed optimization and statistical learni... ([ references, boyd)                                       1\n",
       "learning both weights and connections for effic... ()                                                         2\n",
       "convergence analysis of alternating direction m... (m luo, z hong, ; luo, hong)                               3\n",
       "learning structured sparsity in deep neural net... ()                                                         4\n",
       "...                                                                                                         ...\n",
       "l1-depth revisited: a robust angle-based outlie... (n pham,)                                           62787071\n",
       "locality adaptive discriminant analysis framework  (x li, q wang, f nie, m chen)                       62787072\n",
       "linear discriminant analysis: new formulations ... (d luo, c ding, h huang)                            62787074\n",
       "robust 2dpca with non-greedy ¡inline-formula¿ ¡... (r wang, f nie, x yang, f gao, m yao)               62787079\n",
       "generalization bounds for supervised dimensiona... (m mohri, a rostamizadeh, d storcheus)              62787099\n",
       "\n",
       "[19363735 rows x 1 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.set_index(['title','authors_tup'],inplace=True)\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.to_pickle(\"/home/harsh.d/Shoaib/arxiv_metadata/metadata_title_first_ids.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:myenv] *",
   "language": "python",
   "name": "conda-env-myenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
