{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import lxml\n",
    "from bs4 import BeautifulSoup\n",
    "import ast\n",
    "import pandas as  pd\n",
    "import codecs\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import glob\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading all the xml files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('path_list.pkl', 'rb') as config_dictionary_file:\n",
    "    xml_files = pickle.load(config_dictionary_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1942302"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xml_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_reader(file):\n",
    "    doc = []\n",
    "    with codecs.open(file,'r',\"utf-8\") as tei:\n",
    "        soup = BeautifulSoup(tei, 'lxml')\n",
    "        abstract = soup.teiheader.abstract.get_text().lower().replace('\\n',' ')\n",
    "        title = soup.teiheader.title.get_text().lower().replace('\\n',' ')\n",
    "        full_text = soup.get_text().lower().replace('\\n',' ')\n",
    "        id = file[28:-8]\n",
    "    doc.append({'id':id,'title':title, 'text':full_text,'abstract':abstract})\n",
    "    return doc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text_title_abstract.pkl', 'rb') as f:\n",
    "    df_dic_all_files = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1807.11091</td>\n",
       "      <td>structadmm: a systematic, high-efficiency fram...</td>\n",
       "      <td>structadmm: a systematic, high-efficiency ...</td>\n",
       "      <td>weight pruning methods of dnns have been demo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hep-ph0003173</td>\n",
       "      <td>brane cosmologies without orbifolds</td>\n",
       "      <td>brane cosmologies without orbifolds     ma...</td>\n",
       "      <td>we study the dynamics of branes in configurat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2002.06870</td>\n",
       "      <td>consistency of the plfit estimator for power-l...</td>\n",
       "      <td>consistency of the plfit estimator for pow...</td>\n",
       "      <td>we prove the consistency of the power-law fit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1204.6600</td>\n",
       "      <td>positive operators and maximal operators in a ...</td>\n",
       "      <td>positive operators and maximal operators i...</td>\n",
       "      <td>in a filtered measure space, a characterizati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1404.3331</td>\n",
       "      <td>priors for random count matrices derived from ...</td>\n",
       "      <td>priors for random count matrices derived f...</td>\n",
       "      <td>we define a family of probability distributio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1940905</th>\n",
       "      <td>2011.02495</td>\n",
       "      <td>orbital foregrounds for ultra-short duration t...</td>\n",
       "      <td>orbital foregrounds for ultra-short durati...</td>\n",
       "      <td>reflections from objects in earth orbit can p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1940906</th>\n",
       "      <td>2101.06890</td>\n",
       "      <td>cooperative and competitive biases for multi-a...</td>\n",
       "      <td>cooperative and competitive biases for mul...</td>\n",
       "      <td>training a multi-agent reinforcement learning...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1940907</th>\n",
       "      <td>2112.01060</td>\n",
       "      <td>strong optical coupling in metallo-dielectric ...</td>\n",
       "      <td>strong optical coupling in metallo-dielect...</td>\n",
       "      <td>metasurfaces consisting of hybrid metal/diele...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1940908</th>\n",
       "      <td>2106.06977</td>\n",
       "      <td>floquet engineering of magnetism in topologica...</td>\n",
       "      <td>floquet engineering of magnetism in topolo...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1940909</th>\n",
       "      <td>2202.04678</td>\n",
       "      <td>non-linear spectral dimensionality reduction u...</td>\n",
       "      <td>non-linear spectral dimensionality reducti...</td>\n",
       "      <td>in this paper, we consider the problem of non...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1940910 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                              title  \\\n",
       "0           1807.11091  structadmm: a systematic, high-efficiency fram...   \n",
       "1        hep-ph0003173                brane cosmologies without orbifolds   \n",
       "2           2002.06870  consistency of the plfit estimator for power-l...   \n",
       "3            1204.6600  positive operators and maximal operators in a ...   \n",
       "4            1404.3331  priors for random count matrices derived from ...   \n",
       "...                ...                                                ...   \n",
       "1940905     2011.02495  orbital foregrounds for ultra-short duration t...   \n",
       "1940906     2101.06890  cooperative and competitive biases for multi-a...   \n",
       "1940907     2112.01060  strong optical coupling in metallo-dielectric ...   \n",
       "1940908     2106.06977  floquet engineering of magnetism in topologica...   \n",
       "1940909     2202.04678  non-linear spectral dimensionality reduction u...   \n",
       "\n",
       "                                                      text  \\\n",
       "0            structadmm: a systematic, high-efficiency ...   \n",
       "1            brane cosmologies without orbifolds     ma...   \n",
       "2            consistency of the plfit estimator for pow...   \n",
       "3            positive operators and maximal operators i...   \n",
       "4            priors for random count matrices derived f...   \n",
       "...                                                    ...   \n",
       "1940905      orbital foregrounds for ultra-short durati...   \n",
       "1940906      cooperative and competitive biases for mul...   \n",
       "1940907      strong optical coupling in metallo-dielect...   \n",
       "1940908      floquet engineering of magnetism in topolo...   \n",
       "1940909      non-linear spectral dimensionality reducti...   \n",
       "\n",
       "                                                  abstract  \n",
       "0         weight pruning methods of dnns have been demo...  \n",
       "1         we study the dynamics of branes in configurat...  \n",
       "2         we prove the consistency of the power-law fit...  \n",
       "3         in a filtered measure space, a characterizati...  \n",
       "4         we define a family of probability distributio...  \n",
       "...                                                    ...  \n",
       "1940905   reflections from objects in earth orbit can p...  \n",
       "1940906   training a multi-agent reinforcement learning...  \n",
       "1940907   metasurfaces consisting of hybrid metal/diele...  \n",
       "1940908                                                     \n",
       "1940909   in this paper, we consider the problem of non...  \n",
       "\n",
       "[1940910 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(df_dic_all_files)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopremoveal(t):\n",
    "# for t in tqdm(df_dic_all_files):\n",
    "    word_tokens = t['text'].split()\n",
    "    word_tokens = [ x for x in word_tokens if len(x) >1]\n",
    "    word_tokens = [y for y in word_tokens if not (y.isdigit() or y[0] == '-' and y[1:].isdigit())]\n",
    "    filtered_sentence = [w for w in word_tokens if not w.lower() in stopwords]\n",
    "    filtered_sentence_nonalpha = [re.sub('[^A-Za-z0-9]', ' ', c) for c in filtered_sentence ]\n",
    "    t['text'] = \" \".join(filtered_sentence_nonalpha)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Better to use multiprocessing for this task.\n",
    "for p in tqdm(df_dic_all_files):\n",
    "    stopremoveal(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Inverted Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import multiprocessing\n",
    "\n",
    "def process_file(filename):\n",
    "    # Define the function to process a single file\n",
    "#     with open(filename) as f:\n",
    "#         text = f.read()\n",
    "    text = filename['text']    \n",
    "    # Remove punctuation and convert to lowercase\n",
    "#     text = text.translate(str.maketrans('', '', string.punctuation)).lower()\n",
    "    \n",
    "    # Split the text into a list of words\n",
    "    words = text.split()\n",
    "    \n",
    "    # Initialize an empty list to store the word frequencies and document IDs for this file\n",
    "    word_freq_doc_id = []\n",
    "    \n",
    "    # Loop over each word in the file and update the word frequency and document ID list\n",
    "    for word in tqdm(words):\n",
    "        if len(word) >1:\n",
    "            word_found = False\n",
    "            for i in range(len(word_freq_doc_id)):\n",
    "                if word_freq_doc_id[i][0] == word:\n",
    "                    # If the word is already in the list, increment its frequency\n",
    "                    word_freq_doc_id[i][1] += 1\n",
    "                    word_found = True\n",
    "                    break\n",
    "            if not word_found:\n",
    "                # If the word is not in the list, add it with a frequency of 1 and the file name as the document ID\n",
    "                word_freq_doc_id.append([word, 1, filename['id']])\n",
    "    \n",
    "    # Return the word frequency and document ID list for this file\n",
    "    return word_freq_doc_id\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b07ad8943e944d9082fdf3f935bd218b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/440910 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1182364434649d390c0ceb8d3811df8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/440910 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Define the path to the directory containing the text files\n",
    "    \n",
    "    # Define the number of processes to use\n",
    "    num_processes = 20\n",
    "    \n",
    "    # Create a pool of processes\n",
    "    pool = multiprocessing.Pool(num_processes)\n",
    "    \n",
    "    # Get the list of filenames in the directory\n",
    "#     filenames = [os.path.join(data_dir, filename) for filename in os.listdir(data_dir)]\n",
    "    filenames = df_dic_all_files\n",
    "    with tqdm(total=len(filenames)) as pbar:\n",
    "    # Apply the process_file function to each file using multiple processes\n",
    "        results = pool.map(process_file, filenames)\n",
    "        pbar.update()\n",
    "    \n",
    "    # Initialize an empty dictionary to store the word frequencies and document IDs for all files\n",
    "    word_freq_doc_id = {}\n",
    "    \n",
    "    # Merge the word frequency and document ID lists from each file\n",
    "    for file_result in tqdm(results):\n",
    "        for word, freq, doc_id in file_result:\n",
    "            if word in word_freq_doc_id:\n",
    "                # If the word is already in the dictionary, update its frequency and document ID list\n",
    "                word_freq_doc_id[word]['freq'] += freq\n",
    "                word_freq_doc_id[word]['docs'].append(doc_id)\n",
    "            else:\n",
    "                # If the word is not in the dictionary, add it with a frequency of 1 and a document ID list containing the ID of this file\n",
    "                word_freq_doc_id[word] = {'freq': freq, 'docs': [doc_id]}\n",
    "    \n",
    "    # Print the word frequencies and document IDs for each word\n",
    "#     for word, data in word_freq_doc_id.items():\n",
    "#         print(f\"{word}: {data['freq']} (documents {', '.join(data['docs'])})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('word_freq_doc_id.pkl', 'wb') as f:\n",
    "    pickle.dump(word_freq_doc_id, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:myenv] *",
   "language": "python",
   "name": "conda-env-myenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
