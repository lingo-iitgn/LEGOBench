# LEGOBENCH: Leaderboard Generation Benchmark for Scientific Models

![GitHub](https://img.shields.io/github/license/lingo-iitgn/LEGOBench)
![GitHub release (latest by date)](https://img.shields.io/github/v/release/lingo-iitgn/LEGOBench)
![GitHub last commit](https://img.shields.io/github/last-commit/lingo-iitgn/LEGOBench)



## Abstract

The ever-increasing volume of paper submissions makes it difficult to stay informed about the latest state-of-the-art research. To address this challenge, we introduce **LEGOBENCH**, a benchmark for evaluating systems that generate leaderboards. **LEGOBENCH** is curated from 22 years of preprint submission data in arXiv and more than 11,000 machine learning leaderboards in the PapersWithCode portal. It consists of two tasks: ranking papers and retrieving their scores. Our preliminary results show that traditional graph-based ranking methods and large language models show significant performance gaps in automatic leaderboard generation.

## Features

- **Dataset**: The benchmark dataset used in the paper, curated from arXiv and PapersWithCode.
- **Evaluation Metrics**: Code to evaluate leaderboard generation systems using the proposed metrics.
- **Baseline Methods**: Implementations of baseline methods for leaderboard generation.
- **Experiments**: Code and instructions to reproduce the experiments conducted in the paper.

## Requirements

- Python 3.7 or higher
- Additional requirements can be found in the `requirements.txt` file.

## Usage

1. Clone the repository:

```bash
git clone https://github.com/lingo-iitgn/LEGOBench.git 
```


## Authors

- Shoaib Alam
- Shruti Singh
- Mayank Singh
